{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "tropical-album",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "romance-writer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plugin_write_and_run import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "unlimited-creature",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%write_and_run ../src/mcts.py\n",
    "\n",
    "import math\n",
    "import random\n",
    "from typing import List\n",
    "import numpy\n",
    "\n",
    "from utilities import *\n",
    "from config import *\n",
    "from game import *\n",
    "from networks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "intimate-cooperative",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%write_and_run -a ../src/mcts.py\n",
    "\n",
    "def add_exploration_noise(config: MuZeroConfig, node: Node):\n",
    "    \"\"\"\n",
    "    At the start of each search, we add dirichlet noise to the prior of the root\n",
    "    to encourage the search to explore new actions.\n",
    "    \"\"\"\n",
    "    actions = list(node.children.keys())\n",
    "    noise = numpy.random.dirichlet([config.root_dirichlet_alpha] * len(actions))\n",
    "    frac = config.root_exploration_fraction\n",
    "    for a, n in zip(actions, noise):\n",
    "        node.children[a].prior = node.children[a].prior * (1 - frac) + n * frac\n",
    "\n",
    "\n",
    "def run_mcts(config: MuZeroConfig, root: Node, action_history: ActionHistory, network: SuperNetwork):\n",
    "    \"\"\"\n",
    "    Core Monte Carlo Tree Search algorithm.\n",
    "    To decide on an action, we run N simulations, always starting at the root of\n",
    "    the search tree and traversing the tree according to the UCB formula until we\n",
    "    reach a leaf node.\n",
    "    \"\"\"\n",
    "    min_max_stats = MinMaxStats(config.known_bounds)\n",
    "\n",
    "    for _ in range(config.num_simulations):\n",
    "        history = action_history.clone()\n",
    "        node = root\n",
    "        search_path = [node]\n",
    "\n",
    "        while node.expanded():\n",
    "            action, node = select_child(config, node, min_max_stats)\n",
    "            history.add_action(action)\n",
    "            search_path.append(node)\n",
    "\n",
    "        # Inside the search tree we use the dynamics function to obtain the next\n",
    "        # hidden state given an action and the previous hidden state.\n",
    "        parent = search_path[-2]\n",
    "        network_output = network.recurrent_inference(parent.hidden_state, history.last_action())\n",
    "        expand_node(node, history.to_play(), history.action_space(), network_output)\n",
    "\n",
    "        backpropagate(search_path, network_output.value, history.to_play(), config.discount, min_max_stats)\n",
    "\n",
    "\n",
    "def select_child(config: MuZeroConfig, node: Node, min_max_stats: MinMaxStats):\n",
    "    \"\"\"\n",
    "    Select the child with the highest UCB score.\n",
    "    \"\"\"\n",
    "    # When the parent visit count is zero, all ucb scores are zeros, therefore we return a random child\n",
    "    if node.visit_count == 0:\n",
    "        return random.sample(node.children.items(), 1)[0]\n",
    "\n",
    "    _, action, child = max(\n",
    "        (ucb_score(config, node, child, min_max_stats), action,\n",
    "         child) for action, child in node.children.items())\n",
    "    return action, child\n",
    "\n",
    "\n",
    "def ucb_score(config: MuZeroConfig, parent: Node, child: Node,\n",
    "              min_max_stats: MinMaxStats) -> float:\n",
    "    \"\"\"\n",
    "    The score for a node is based on its value, plus an exploration bonus based on\n",
    "    the prior.\n",
    "    \"\"\"\n",
    "    pb_c = math.log((parent.visit_count + config.pb_c_base + 1) / config.pb_c_base) + config.pb_c_init\n",
    "    pb_c *= math.sqrt(parent.visit_count) / (child.visit_count + 1)\n",
    "\n",
    "    prior_score = pb_c * child.prior\n",
    "    value_score = min_max_stats.normalize(child.value())\n",
    "    return prior_score + value_score\n",
    "\n",
    "\n",
    "def expand_node(node: Node, to_play: Player, actions: List[Action],\n",
    "                network_output: NetworkOutput):\n",
    "    \"\"\"\n",
    "    We expand a node using the value, reward and policy prediction obtained from\n",
    "    the neural networks.\n",
    "    \"\"\"\n",
    "    node.to_play = to_play\n",
    "    node.hidden_state = network_output.hidden_state\n",
    "    node.reward = network_output.reward\n",
    "    policy = {a: math.exp(network_output.policy_logits[a]) for a in actions}\n",
    "    policy_sum = sum(policy.values())\n",
    "    for action, p in policy.items():\n",
    "        node.children[action] = Node(p / policy_sum)\n",
    "\n",
    "\n",
    "def backpropagate(search_path: List[Node], value: float, to_play: Player,\n",
    "                  discount: float, min_max_stats: MinMaxStats):\n",
    "    \"\"\"\n",
    "    At the end of a simulation, we propagate the evaluation all the way up the\n",
    "    tree to the root.\n",
    "    \"\"\"\n",
    "    for node in search_path[::-1]:\n",
    "        node.value_sum += value if node.to_play == to_play else -value\n",
    "        node.visit_count += 1\n",
    "        min_max_stats.update(node.value())\n",
    "\n",
    "        value = node.reward + discount * value\n",
    "\n",
    "\n",
    "def select_action(config: MuZeroConfig, num_moves: int, node: Node, network: SuperNetwork, mode: str = 'softmax'):\n",
    "    \"\"\"\n",
    "    After running simulations inside in MCTS, we select an action based on the root's children visit counts.\n",
    "    During training we use a softmax sample for exploration.\n",
    "    During evaluation we select the most visited child.\n",
    "    \"\"\"\n",
    "    visit_counts = [child.visit_count for child in node.children.values()]\n",
    "    actions = [action for action in node.children.keys()]\n",
    "    action = None\n",
    "    if mode == 'softmax':\n",
    "        t = config.visit_softmax_temperature_fn(\n",
    "            num_moves=num_moves, training_steps=network.training_steps)\n",
    "        action = softmax_sample(visit_counts, actions, t)\n",
    "    elif mode == 'max':\n",
    "        action, _ = max(node.children.items(), key=lambda item: item[1].visit_count)\n",
    "    return action"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
