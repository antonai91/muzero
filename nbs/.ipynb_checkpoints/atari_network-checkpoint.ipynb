{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "simplified-foster",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "worthy-parade",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plugin_write_and_run import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "quick-analyst",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%write_and_run ../src/atari_network.py\n",
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from utilities import *\n",
    "from config import *\n",
    "from game import *\n",
    "from networks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "julian-chapel",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%write_and_run -a ../src/atari_network.py\n",
    "\n",
    "class CartPoleNetwork(SuperNetwork):\n",
    "\n",
    "    def __init__(self,\n",
    "                 state_size: int,\n",
    "                 action_size: int,\n",
    "                 representation_size: int,\n",
    "                 max_value: int,\n",
    "                 hidden_neurons: int = 64,\n",
    "                 weight_decay: float = 1e-4,\n",
    "                 representation_activation: str = 'tanh'):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.value_support_size = math.ceil(math.sqrt(max_value)) + 1\n",
    "\n",
    "        regularizer = regularizers.l2(weight_decay)\n",
    "        representation_network = Sequential([Dense(hidden_neurons, activation='relu', kernel_regularizer=regularizer),\n",
    "                                             Dense(representation_size, activation=representation_activation,\n",
    "                                                   kernel_regularizer=regularizer)])\n",
    "        value_network = Sequential([Dense(hidden_neurons, activation='relu', kernel_regularizer=regularizer),\n",
    "                                    Dense(self.value_support_size, kernel_regularizer=regularizer)])\n",
    "        policy_network = Sequential([Dense(hidden_neurons, activation='relu', kernel_regularizer=regularizer),\n",
    "                                     Dense(action_size, kernel_regularizer=regularizer)])\n",
    "        dynamic_network = Sequential([Dense(hidden_neurons, activation='relu', kernel_regularizer=regularizer),\n",
    "                                      Dense(representation_size, activation=representation_activation,\n",
    "                                            kernel_regularizer=regularizer)])\n",
    "        reward_network = Sequential([Dense(16, activation='relu', kernel_regularizer=regularizer),\n",
    "                                     Dense(1, kernel_regularizer=regularizer)])\n",
    "\n",
    "        super().__init__(representation_network, value_network, policy_network, dynamic_network, reward_network)\n",
    "\n",
    "    def _value_transform(self, value_support: np.array) -> float:\n",
    "        \"\"\"\n",
    "        The value is obtained by first computing the expected value from the discrete support.\n",
    "        Second, the inverse transform is then apply (the square function).\n",
    "        \"\"\"\n",
    "\n",
    "        value = self._softmax(value_support)\n",
    "        value = np.dot(value, range(self.value_support_size))\n",
    "        value = np.asscalar(value) ** 2\n",
    "        return value\n",
    "\n",
    "    def _reward_transform(self, reward: np.array) -> float:\n",
    "        return np.asscalar(reward)\n",
    "\n",
    "    def _conditioned_hidden_state(self, hidden_state: np.array, action: Action) -> np.array:\n",
    "        conditioned_hidden = np.concatenate((hidden_state, np.eye(self.action_size)[action.index]))\n",
    "        return np.expand_dims(conditioned_hidden, axis=0)\n",
    "\n",
    "    def _softmax(self, values):\n",
    "        \"\"\"Compute softmax using numerical stability tricks.\"\"\"\n",
    "        values_exp = np.exp(values - np.max(values))\n",
    "        return values_exp / np.sum(values_exp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
