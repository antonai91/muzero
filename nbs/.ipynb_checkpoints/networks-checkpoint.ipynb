{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "handed-relationship",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "downtown-irish",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plugin_write_and_run import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "infinite-oakland",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%write_and_run ../src/networks.py\n",
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "from typing import Dict, List, Callable, NamedTuple\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from game import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "patient-survival",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%write_and_run -a ../src/networks.py\n",
    "\n",
    "class NetworkOutput(NamedTuple):\n",
    "    value: float\n",
    "    reward: float\n",
    "    policy_logits: Dict[Action, float]\n",
    "    hidden_state: List[float]\n",
    "\n",
    "    @staticmethod\n",
    "    def build_policy_logits(policy_logits):\n",
    "        return {Action(i): logit for i, logit in enumerate(policy_logits[0])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "original-pilot",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%write_and_run -a ../src/networks.py\n",
    "\n",
    "class UniformNetwork():\n",
    "    \"\"\"policy -> uniform, value -> 0, reward -> 0\"\"\"\n",
    "\n",
    "    def __init__(self, action_size: int):\n",
    "        super().__init__()\n",
    "        self.action_size = action_size\n",
    "        self.training_steps = 0\n",
    "\n",
    "    def initial_inference(self, image) -> NetworkOutput:\n",
    "        return NetworkOutput(0, 0, {Action(i): 1 / self.action_size for i in range(self.action_size)}, None)\n",
    "\n",
    "    def recurrent_inference(self, hidden_state, action) -> NetworkOutput:\n",
    "        return NetworkOutput(0, 0, {Action(i): 1 / self.action_size for i in range(self.action_size)}, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "empty-viewer",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%write_and_run -a ../src/networks.py\n",
    "\n",
    "class InitialModel(tf.keras.Model):\n",
    "    \"\"\"Model that combine the representation and prediction (value+policy) network.\"\"\"\n",
    "\n",
    "    def __init__(self, representation_network: tf.keras.Model, value_network: tf.keras.Model, policy_network: tf.keras.Model):\n",
    "        super(InitialModel, self).__init__()\n",
    "        self.representation_network = representation_network\n",
    "        self.value_network = value_network\n",
    "        self.policy_network = policy_network\n",
    "\n",
    "    def call(self, image):\n",
    "        hidden_representation = self.representation_network(image)\n",
    "        value = self.value_network(hidden_representation)\n",
    "        policy_logits = self.policy_network(hidden_representation)\n",
    "        return hidden_representation, value, policy_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "physical-movie",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%write_and_run -a ../src/networks.py\n",
    "\n",
    "class RecurrentModel(tf.keras.Model):\n",
    "    \"\"\"Model that combine the dynamic, reward and prediction (value+policy) network.\"\"\"\n",
    "\n",
    "    def __init__(self, dynamic_network: tf.keras.Model, reward_network: tf.keras.Model, value_network: tf.keras.Model, policy_network: tf.keras.Model):\n",
    "        super(RecurrentModel, self).__init__()\n",
    "        self.dynamic_network = dynamic_network\n",
    "        self.reward_network = reward_network\n",
    "        self.value_network = value_network\n",
    "        self.policy_network = policy_network\n",
    "\n",
    "    def call(self, conditioned_hidden):\n",
    "        hidden_representation = self.dynamic_network(conditioned_hidden)\n",
    "        reward = self.reward_network(conditioned_hidden)\n",
    "        value = self.value_network(hidden_representation)\n",
    "        policy_logits = self.policy_network(hidden_representation)\n",
    "        return hidden_representation, reward, value, policy_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "realistic-access",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%write_and_run -a ../src/networks.py\n",
    "\n",
    "class SuperNetwork():\n",
    "    \"\"\"Super class that contains all the networks and models of MuZero.\"\"\"\n",
    "\n",
    "    def __init__(self, representation_network: tf.keras.Model, value_network: tf.keras.Model, policy_network: tf.keras.Model,\n",
    "                 dynamic_network: tf.keras.Model, reward_network: tf.keras.Model):\n",
    "        super().__init__()\n",
    "        self.training_steps = 0\n",
    "        # Networks blocks\n",
    "        self.representation_network = representation_network\n",
    "        self.value_network = value_network\n",
    "        self.policy_network = policy_network\n",
    "        self.dynamic_network = dynamic_network\n",
    "        self.reward_network = reward_network\n",
    "\n",
    "        # Models for inference and training\n",
    "        self.initial_model = InitialModel(self.representation_network, self.value_network, self.policy_network)\n",
    "        self.recurrent_model = RecurrentModel(self.dynamic_network, self.reward_network, self.value_network,\n",
    "                                              self.policy_network)\n",
    "\n",
    "    def initial_inference(self, image: np.array) -> NetworkOutput:\n",
    "        \"\"\"representation + prediction function\"\"\"\n",
    "\n",
    "        hidden_representation, value, policy_logits = self.initial_model.predict(np.expand_dims(image, 0))\n",
    "        output = NetworkOutput(value=self._value_transform(value),\n",
    "                               reward=0.,\n",
    "                               policy_logits=NetworkOutput.build_policy_logits(policy_logits),\n",
    "                               hidden_state=hidden_representation[0])\n",
    "        return output\n",
    "\n",
    "    def recurrent_inference(self, hidden_state: np.array, action: Action) -> NetworkOutput:\n",
    "        \"\"\"dynamics + prediction function\"\"\"\n",
    "\n",
    "        conditioned_hidden = self._conditioned_hidden_state(hidden_state, action)\n",
    "        hidden_representation, reward, value, policy_logits = self.recurrent_model.predict(conditioned_hidden)\n",
    "        output = NetworkOutput(value=self._value_transform(value),\n",
    "                               reward=self._reward_transform(reward),\n",
    "                               policy_logits=NetworkOutput.build_policy_logits(policy_logits),\n",
    "                               hidden_state=hidden_representation[0])\n",
    "        return output\n",
    "    \n",
    "    def cb_get_variables(self) -> Callable:\n",
    "        \"\"\"Return a callback that return the trainable variables of the network.\"\"\"\n",
    "\n",
    "        def get_variables():\n",
    "            networks = (self.representation_network, self.value_network, self.policy_network,\n",
    "                        self.dynamic_network, self.reward_network)\n",
    "            return [variables\n",
    "                    for variables_list in map(lambda n: n.weights, networks)\n",
    "                    for variables in variables_list]\n",
    "\n",
    "        return get_variables"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
