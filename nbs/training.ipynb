{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plugin_write_and_run import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%write_and_run ../src/training.py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from utilities import *\n",
    "from config import *\n",
    "from game import *\n",
    "from shared_storage import *\n",
    "from networks import *\n",
    "from mcts import *\n",
    "from self_play import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%write_and_run -a ../src/training.py\n",
    "\n",
    "def train_network(config: MuZeroConfig, storage: SharedStorage, replay_buffer: ReplayBuffer, epochs: int):\n",
    "    network = storage.current_network\n",
    "    optimizer = storage.optimizer\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        batch = replay_buffer.sample_batch(config.num_unroll_steps, config.td_steps)\n",
    "        update_weights(optimizer, network, batch)\n",
    "        storage.save_network(network.training_steps, network)\n",
    "        \n",
    "def update_weights(optimizer: tf.keras.optimizers, network: SuperNetwork, batch):\n",
    "    def scale_gradient(tensor, scale: float):\n",
    "        \"\"\"Trick function to scale the gradient in tensorflow\"\"\"\n",
    "        return (1. - scale) * tf.stop_gradient(tensor) + scale * tensor\n",
    "    \n",
    "    def loss():\n",
    "        loss = 0\n",
    "        image_batch, targets_init_batch, targets_time_batch, actions_time_batch, mask_time_batch, dynamic_mask_time_batch = batch\n",
    "\n",
    "        # Initial step, from the real observation: representation + prediction networks\n",
    "        representation_batch, value_batch, policy_batch = network.initial_model(np.array(image_batch))\n",
    "\n",
    "        # Only update the element with a policy target\n",
    "        target_value_batch, _, target_policy_batch = zip(*targets_init_batch)\n",
    "        mask_policy = list(map(lambda l: bool(l), target_policy_batch))\n",
    "        target_policy_batch = list(filter(lambda l: bool(l), target_policy_batch))\n",
    "        policy_batch = tf.boolean_mask(policy_batch, mask_policy)\n",
    "\n",
    "        # Compute the loss of the first pass\n",
    "        loss += tf.math.reduce_mean(loss_value(target_value_batch, value_batch, network.value_support_size))\n",
    "        loss += tf.math.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(logits=policy_batch, labels=target_policy_batch))\n",
    "\n",
    "        # Recurrent steps, from action and previous hidden state.\n",
    "        for actions_batch, targets_batch, mask, dynamic_mask in zip(actions_time_batch, targets_time_batch,\n",
    "                                                                    mask_time_batch, dynamic_mask_time_batch):\n",
    "            target_value_batch, target_reward_batch, target_policy_batch = zip(*targets_batch)\n",
    "\n",
    "            # Only execute BPTT for elements with an action\n",
    "            representation_batch = tf.boolean_mask(representation_batch, dynamic_mask)\n",
    "            target_value_batch = tf.boolean_mask(target_value_batch, mask)\n",
    "            target_reward_batch = tf.boolean_mask(target_reward_batch, mask)\n",
    "            # Creating conditioned_representation: concatenate representations with actions batch\n",
    "            actions_batch = tf.one_hot(actions_batch, network.action_size)\n",
    "\n",
    "            # Recurrent step from conditioned representation: recurrent + prediction networks\n",
    "            conditioned_representation_batch = tf.concat((representation_batch, actions_batch), axis=1)\n",
    "            representation_batch, reward_batch, value_batch, policy_batch = network.recurrent_model(\n",
    "                conditioned_representation_batch)\n",
    "\n",
    "            # Only execute BPTT for elements with a policy target\n",
    "            target_policy_batch = [policy for policy, b in zip(target_policy_batch, mask) if b]\n",
    "            mask_policy = list(map(lambda l: bool(l), target_policy_batch))\n",
    "            target_policy_batch = tf.convert_to_tensor([policy for policy in target_policy_batch if policy])\n",
    "            policy_batch = tf.boolean_mask(policy_batch, mask_policy)\n",
    "\n",
    "            # Compute the partial loss\n",
    "            l = (tf.math.reduce_mean(loss_value(target_value_batch, value_batch, network.value_support_size)) +\n",
    "                 tf.keras.losses.MSE(target_reward_batch, tf.squeeze(reward_batch)) +\n",
    "                 tf.math.reduce_mean(\n",
    "                     tf.nn.softmax_cross_entropy_with_logits(logits=policy_batch, labels=target_policy_batch)))\n",
    "\n",
    "            # Scale the gradient of the loss by the average number of actions unrolled\n",
    "            gradient_scale = 1. / len(actions_time_batch)\n",
    "            loss += scale_gradient(l, gradient_scale)\n",
    "\n",
    "            # Half the gradient of the representation\n",
    "            representation_batch = scale_gradient(representation_batch, 0.5)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    optimizer.minimize(loss=loss, var_list=network.cb_get_variables())\n",
    "    network.training_steps += 1\n",
    "    \n",
    "def loss_value(target_value_batch, value_batch, value_support_size: int):\n",
    "    batch_size = len(target_value_batch)\n",
    "    targets = np.zeros((batch_size, value_support_size))\n",
    "    sqrt_value = np.sqrt(target_value_batch)\n",
    "    floor_value = np.floor(sqrt_value).astype(int)\n",
    "    rest = sqrt_value - floor_value\n",
    "    targets[range(batch_size), floor_value.astype(int)] = 1 - rest\n",
    "    targets[range(batch_size), floor_value.astype(int) + 1] = rest\n",
    "\n",
    "    return tf.nn.softmax_cross_entropy_with_logits(logits=value_batch, labels=targets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
